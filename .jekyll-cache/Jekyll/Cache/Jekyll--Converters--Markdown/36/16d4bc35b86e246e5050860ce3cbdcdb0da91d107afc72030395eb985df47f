I"í<p>Stochastic gradieng (SG) methods are currently the major optimization method being used for deep learning because of the simplicity and efficiency. However, the method is sometimes sensitive to parameters and their tuning can be a painful process. To alleviate the above issues, we study the Newtonâ€™s method for training Deep Neural Networks (DNN), which can not only diminish the need for hyper-parameter tuning but also emjoy the benefit of fast convergence according to theoretical analysis.</p>

<p>We have developed a <a href="https://github.com/cjlin1/simpleNN">toolkit</a> in Tensorflow and MATLAB for DNNs using Newton-CG methods. For the core operation of Gauss-Newton matrix-vector products, I use Tensorflowâ€™s vector-Jacobian products. See implementation details in this <a href="/assets/pdf/Calculating_Gauss_Newton_Matrix_Vector_product_by_Vector_Jacobian_Products.pdf">document</a>.</p>

<p>I also gave a <a href="/assets/pdf/Newton_methods.pdf">presentation</a> on this project.</p>
:ET